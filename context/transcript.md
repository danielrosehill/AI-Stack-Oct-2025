## Introduction: The Purpose of This Recording
Okay, so the objective of this recording is to provide some answers to questions asked of me by a large language model, as people do these days. The goal of this is to transcribe this and add it into my readme because, once or twice a year, usually around this time in winter, I try to organize all my GitHub repositories. I condense, I delete some old ones, and I also create some new ones, so thereâ€™s been a bit of a burst of activity for the past few days.

One thing that I want to do every now and again is a stack overview of my AI stack. This is for several reasons. Firstly, friends ask me. Sometimes for clients, they're kind of wondering, with so much on the market, what do you think is good? And I also like to note it down because it's a fluid stack. For anything, it's a work in progress or an evolving entity. I like to just take notes on what I'm adding, what I'm dropping, and why I find something valuable. So that's the objective here. It's just to record, probably a once-a-year exercise, but the pace of evolution is so rapid that you could very easily version control your stack every month if you wanted to do it like that. But this is the update.

## The Interview Setup
So, in response, I jotted down the tools I'm using at the moment and just some notes. I created, and I thought this was a clever use, I created `claude.md` for this repo, and I said, "Firstly, clean up the mess of my notes," which is always, it's very good at that, "and also, interview me." So, it interviewed me by asking this set of questions I'm going to answer. I'm using it as a jumping-off point.

## Core AI Infrastructure: Providers and Models
Let's start with the foundation. What's your primary LLM provider? Are you using LLM routers or aggregators? Do you run any local models via Ollama? Okay, so my primary LLM provider is probably OpenRouter. Although, one thing I'm never sure about is whether you're getting the same quality of inference as you do when you're using the vendor. So for Claude, which I've rapidly been enamored by, it seems to be the best performance from Anthropic as opposed to using it through, let's say, Winserf or OpenRouter. I assume that there is some reason for that and it's not just my imagination. So I think there's always a case for first-tenant access.

But the main reason, for someone who's just sort of maybe looking to go beyond ChatGPT, which is also why I jotted down the stack notes, it would be one of the first things I recommend: set up an account on OpenRouter. Whether you're looking into self-hosting a chat interface like Open WebUI or whether you're just looking for it programmatically, to integrate. And that's really where AI becomes exciting and cool, is when you see that instructional AI and programmatic AI is a very mature thing and brings independent value over chatbots. You can do a whole bunch of stuff. So having an account with them is a no-brainer in my opinion. You would literally open an account and drop in 50 bucks if you can afford that, just to have a balance on hand and give yourself room to play around with, although they do offer some free inference as well.

The reason I like OpenRouter is, firstly, expense consolidation. I have my own business, and that's a reason that I don't see frequently articulated. When you have to provide your expenses to your accountant, it's actually a lot easier to just have one API bill than it is to have little bits and pieces coming at you. I think from budgeting, it probably actually makes sense as well. It's very easy to start spending 20 bucks here and there on APIs and lose track of what you're actually spending. Whereas when you've got one, you can say, "Okay, this is my OpenRouter account. I'm going to set my spend limits here." There's a significant reason there why that's a good choice.

The second reason is very much different to that, is that you can just try out a lot of different models. So I ran an evaluation today called something like "AI Brevity" because I'm frequently frustrated by the verbosity of conventional AI models. And equally, when you try to system prompt it like, "Just be super concise," or "Answer in two words," you don't get good results. So I thought, okay, let's just run this prompt through 10 models. Now, I did that with OpenRouter and a script, but OpenRouter is the logical entry way because you've got all the models available there and you just need to change out the model in your evaluation.

There's a lot more than just OpenAI, Anthropic, Gemini, etc. It's just interesting to keep an eye on, especially what's coming out of China, and what's emerging sort of at the frontier of what could be fine-tuned. There's significant barriers to entry in bringing any truly novel model to market, but there are, for example, a lot of models that just never make the news, like IBM Granite, Amazon's models, Microsoft Phi. And some of these are particularly good at instruction following, which is something that I frequently find myself looking for. I find actually that the most transformative and robust uses for large language models at the moment are frequently just simple text transformations. For that, you don't need or want a very, very high-reasoning model. You want something that's just good at, "Okay, this is the system prompt, this is your task, go from A to B." And accessing those models, you can do with serverless inference on demand, but stack consolidation is always the order of the day. And being able to have a mix of open-source models and commercial models all through the same API is a big advantage.

## Local AI with Ollama
Then regarding local AI, I do run Ollama with a GPU. It's an AMD GPU, which kind of limits things a bit. My first recommendation is to go with an Nvidia GPU. I wasn't quite as deep into this stuff as I am now when I was buying this computer, but if I were repeating the process, I would have gone Nvidia. Not that you can't do AI workloads with AMD, but it's just another layer of difficulty you're adding to the process.

I'm not ever sure with Ollama if there's really much point because I'm not doing it from a privacy, for any privacy reasons. I'm not even doing it for cost-saving reasons. The cost of speech-to-text with Whisper is not very much. It's very good value, I would argue. And the cost of inference on cheaper models is also really good value, so it's not really a compelling reason for me to try to save a couple of dollars here and there by doing stuff locally. It's more a question of speed. I'd love to get an agentic, like a local agent, and that's why GLM, their model 4.6, is interesting to me. I've tried using Qwen models for code generation locally and for agentic work locally, and I just haven't found they work well enough to justify using them.

I've explored it and I'm very eager to continue exploring that world, but I keep, even for image generation, just coming back to, I'm just going to use something in the cloud for this. So I have it, but I use it sparingly. Probably the most successful results I've seen so far have been from these big batch jobs, like text confirmation again, like Meta Llama 3.1. When you might have like, let's say one of the things I'm working on at the moment is voice notes and trying to create a classification model for those from my own idea that I'm working on. For a task like that, we might have thousands of Markdown files to iterate through. That's where like setting up a batch job on local for me has proven worthwhile. You don't have concerns with rate limiting, you can just leave it. But yeah, it's been really quite marginal for me so far.

## Primary Interaction Methods and Tools
How do I primarily interact with AI? Claude Code. So it's funny, I would say actually ChatGPT is just kind of like, I self-hosted Open WebUI for a while and then I just kind of, again, reached the conclusion that before I became a father and got busy, I didn't see any compelling reason to have the hassle of stuff breaking and having to fix it.

Claude Code, I saw someone on Hugging Face say that Claude Code has been the greatest thing they've found since ChatGPT, and I was like, "Oh my gosh, great, it's not just me," because it's been the same. I feel the same way about it. I've been using Linux for like 20 years, and CLIs, you know, are kind of second nature-ish, but I always try, if there is a GUI, I prefer to use that than a CLI. But when I began using Claude's Code and Gemini CLI and CodeX, from the very start, I don't understand why there isn't a bigger movement to use these things for local just computer management. There was a project called Open Interpreter that was good and had a lot of potential. And when I started using Claude Code, I was like, wait, I can just pretty much do everything that I need on Linux. I would describe Linux, from many years of using it, it's certainly gotten easier over time, but there's always a list of things that don't quite work, bugs, or things I wish I could do. And it's the first time I've ever had almost an empty list because every time I have a problem like that, I'm like, "Hey Claude, any chance you can figure out why?" Like, create a shortcut for turning the screens on and off, or why is this not working? And it's just blown through all of those things. And then I use it as well for local sysadmin, and it's my, I hate to call it a home lab because I never really intended to get into home labbing, but my local network, and it just fixes things on the remotes in record time.

## A Review of My AI Tool Stack
Now I'm going to just go through the list of things I use. We've covered LLMs, we've covered MCP.

**Speech-to-Text (STT):** For STT, Whisper is just solid. OpenAI brought out a new model and I haven't seen it actually do better in my own workloads. AssemblyAI is great. I think there's a lot of value in diarization. It's very helpful and it's a foundational element if you use something like an AI tool that processes a meeting transcript and extracts minutes. You'd want to start from a good transcript and a diarized transcript, and that's where I frequently find these more useful than the generic ones. I think it's a mistake to use, to expect that the STT tool you use for live text transformation is going to work for asynchronous jobs like transcribing a meeting. There are significant differences in performance within STT tools. I've jotted down in the repository here: AssemblyAI, Deepgram, Speechmatics, and Gladio I would add, are very solid.

**Text-to-Speech (TTS):** I've got Lemonfox for super cheap TTS. For TTS, I describe ElevenLabs as the best in class, but it's very expensive. So I have here a budget-friendly option, OpenAI TTS, and I think it's good enough for the majority of things. Obviously, with ElevenLabs you get really nice expressiveness, but yeah, it's a case of if you can pay for it. And that's why when I get around to modeling out some budgets here, I can imagine maybe sharing this with clients at some point, being like, within these modalities, speech-to-text, there is, if you're not concerned about saving dollars, I would recommend ElevenLabs. If you are concerned about saving dollars, I would just use OpenAI mini or something else.

**Generative AI:** I'm massively impressed by Replicate and I'm massively impressed by Fal. They're both absolutely brilliant. And again, it comes back to the same thing with OpenRouter. It's not just that it's very convenient to run generative AI tasks like text-to-video using an API, and instead of one API and you can just top up that. It's also useful because you can explore all the models they have and you're like, "Oh my gosh, I never knew there was even such a thing as audio inpainting." Like, wait, that exists? And like, okay, who are these companies? And like, let's try them all out with one task before deciding this is the best one and I'm going to spend a few hundred dollars on this workload. So Replicate and Fal. Fal has a little bit more exposure to the Chinese models from what I can see. I don't really have majorly strong opinions about which is the better one, other than that I'm amazed by the pace at which new generative AI modalities and models are coming to market on those platforms. It's staggering.

**Supporting Tools and Platforms:** I also use Modal and RunPod, Hugging Face Spaces for deploying privately. And then Vercel. I recently migrated from Netlify, just finding it a bit more easier to use. They have a bit more AI forward, and environment variable sharing across projects is a really nice touch.

**Retrieval-Augmented Generation (RAG):** For RAG, I really like Supermemory's philosophy of "don't build your own RAG pipeline" because every time I try to use something like Pinecone, you're like, "Well, okay, first I need to embed this. What vector size? What chunking?" And I'm like, wait, I'm just trying to have a few documents that an AI can ground itself in. You can go down that rabbit hole for a few hours setting up Qdrant, setting up Pinecone, and it feels to me like unless you have a very, very significant enterprise-level store of material, that's not a good use of time for most developers who are not working at that level of scale. That's where I really like their Supermemory RAG, all the memory layer ones. I really like that philosophy. Like, don't waste your time building out this whole retrieval engineering.

Firecrawl is another tool I just have a note here because there's a whole world of MCP tools that are really useful. Markdown is just such a useful format for AI workloads, and the ability just to quickly say, "Okay, these are the API docs for something that you can clearly see you're struggling with," and you know, you might find there's Context, which is a great MCP. But I'm just going to grab this URL because I can see this is exactly what you need. Oh wait, I need this in Markdown. You can just have one tool that you know can do this over and over reliably, it is extremely useful. So it's a great start and the ability to define the models. I think scraping is almost a bad word and a dirty word because people immediately assume it's spammy and illegal or whatever. Whereas I've often used these to scrape my own stuff. If it's notes, I'm like, oh, I wrote these notes a few years ago. I can just use Firecrawl to pull this in. It's actually quicker to do that than to go back through my Google Drive from four years ago and then format that in Markdown. So it's useful across those domains.

## The Power of Deliberate Context Generation
Finally, NotebookLM has really come to maturity. I understand why people are so excited about it now. When I last tried it, it was like 10 sources. It's one of those tools that you could do in another way, you could use LlamaIndex, but being able to just quickly assemble those building blocks of retrieval, questions, and then generations based on retrieval, it does it really well. I just realized that it's actually perfect for a pattern I've been working on for a while, which is, I was going to call it, "AI, I have a problem." And I realized that's probably not going to be the best marketing name that I've ever come up with. But the pattern was, if I have a problem to solve, I'm not asking for like a pasta recipe. The example is a problem I've been trying to solve for years. I had surgery a number of years ago, gallbladder surgery, and I have these really bad issues with all manner of digestion, bloating, nausea, you name it. And I don't really enjoy narrating the whole story, but if I can do that once, have that context data, and then use a workspace to come up with shopping lists that are better for me and recipes, that's what I meant by that pattern. That, I find very, very effective. And if you're working on a few of these problems, quote-unquote, you really see the advantage of this approach over time as opposed to repetitively defining context.

There's another significant advantage to this approach, in my opinion. I've done some modeling in interview-first context generation, or what I've called deliberate context generation. Which is why I've become a big fan of these voice workflows because I can record a voice note like this and in 30 minutes gather a huge amount of information, put that into a little transformation like extracting the context data from that, and then that's the starting context. It's really working in reverse to the way that, let's say, OpenAI are doing it, which is you chat with ChatGPT and then over time, it learns things about you and extracts that to a memory layer. This is working backwards. I'm not saying that that's wrong or that my approach is better at all, I'm just saying that it's a different method for really trying to tackle the same question of how we can see that AI is much more useful when it's personalized to you and your particular life circumstances. How can we engineer that? That's my approach. I've done some more and less experimental ones. The basic one is just this, I speak for 30 minutes into a microphone. The slightly jazzier one is a bot will ask me questions like a model interview. And that's another workflow that actually has worked very well. And sort of what I'm doing here, I guess, the bot came up with the questions and there you go, I can speak for 30 minutes. So that's my context data for my AI stack for October 2025.